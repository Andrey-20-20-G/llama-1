import os
import fitz  # PyMuPDF
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM
from peft import AutoPeftModelForCausalLM
import tempfile

MODEL_PATH = "models/llama3-finetuned"  # –ü—É—Ç—å –∫ –¥–æ–æ–±—É—á–µ–Ω–Ω–æ–π LLaMA-3 –∏–ª–∏ OpenChat


def extract_text_from_pdf(pdf_path: str) -> str:
    """
    –ò–∑–≤–ª–µ–∫–∞–µ—Ç –≤–µ—Å—å —Ç–µ–∫—Å—Ç –∏–∑ PDF —Ñ–∞–π–ª–∞
    """
    print(f"[PDF] –ò–∑–≤–ª–µ–∫–∞–µ–º —Ç–µ–∫—Å—Ç –∏–∑ —Ñ–∞–π–ª–∞: {pdf_path}")
    doc = fitz.open(pdf_path)
    full_text = ""

    for page in doc:
        full_text += page.get_text()

    doc.close()
    return full_text.strip()


def build_prompt(text: str) -> list:
    """
    –ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç chat-–ø–æ–¥–æ–±–Ω—ã–π prompt –≤ —Ñ–æ—Ä–º–∞—Ç–µ –æ–±—É—á–µ–Ω–∏—è
    """
    return [
        {
            "role": "user",
            "content": f"–û–ø—Ä–µ–¥–µ–ª–∏, –∫—Ç–æ –Ω–∞–ø–∏—Å–∞–ª —Å–ª–µ–¥—É—é—â–∏–π —Ç–µ–∫—Å—Ç:\n\n{text}\n\n–û—Ç–≤–µ—Ç:"
        }
    ]


# def load_model(model_path: str):
#     tokenizer = AutoTokenizer.from_pretrained(model_path, use_fast=True)
#     model = AutoModelForCausalLM.from_pretrained(
#         model_path,
#         torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,
#         device_map="auto"
#     )
#     return tokenizer, model

# def load_model(model_path):
#     tokenizer = AutoTokenizer.from_pretrained(model_path, use_fast=True)

#     # –°–æ–∑–¥–∞–µ–º –≤—Ä–µ–º–µ–Ω–Ω—É—é –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –¥–ª—è offload (–∏–ª–∏ —É–∫–∞–∂–∏ —Å–≤–æ—é)
#     offload_dir = tempfile.mkdtemp()

#     print(f"[‚öôÔ∏è] –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è offload_dir: {offload_dir}")

#     model = AutoModelForCausalLM.from_pretrained(
#         model_path,
#         device_map="auto",
#         torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,
#         offload_folder=offload_dir,
#         offload_state_dict=True
#     )

#     return tokenizer, model

def load_model(model_path):
    tokenizer = AutoTokenizer.from_pretrained(model_path, use_fast=True)

    model = AutoModelForCausalLM.from_pretrained(
        model_path,
        device_map="auto",
        torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,
        low_cpu_mem_usage=True
    )

    return tokenizer, model


def predict_from_pdf(pdf_path: str):
    text = extract_text_from_pdf(pdf_path)

    if len(text) < 200:
        raise ValueError("‚ùå –°–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∏–π —Ç–µ–∫—Å—Ç –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞.")

    print(f"[INFO] –î–ª–∏–Ω–∞ —Å—Ç–∞—Ç—å–∏: {len(text)} —Å–∏–º–≤–æ–ª–æ–≤")

    tokenizer, model = load_model(MODEL_PATH)

    messages = build_prompt(text)
    prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)

    input_ids = tokenizer(prompt, return_tensors="pt").input_ids.to(model.device)

    print("[üß†] –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –æ—Ç–≤–µ—Ç...")

    output = model.generate(
        input_ids=input_ids,
        max_new_tokens=50,
        do_sample=False,
        temperature=0.0
    )

    decoded = tokenizer.decode(output[0], skip_special_tokens=True)

    # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ç–æ–ª—å–∫–æ —á–∞—Å—Ç—å –ø–æ—Å–ª–µ prompt
    generated_part = decoded.split(messages[0]["content"])[-1].strip()

    print("\nüìú –í–æ–ø—Ä–æ—Å –∫ –º–æ–¥–µ–ª–∏:")
    print(messages[0]["content"])
    print("\nü§ñ –û—Ç–≤–µ—Ç –º–æ–¥–µ–ª–∏:")
    print(generated_part)


if __name__ == "__main__":
    import argparse

    parser = argparse.ArgumentParser(description="LLama-–∏–Ω—Ñ–µ—Ä–µ–Ω—Å –ø–æ PDF —Å—Ç–∞—Ç—å–µ")
    parser.add_argument("--pdf", type=str, required=True, help="–ü—É—Ç—å –∫ —Å—Ç–∞—Ç—å–µ —Ñ–æ—Ä–º–∞—Ç–∞ PDF")
    args = parser.parse_args()

    if not os.path.exists(args.pdf) or not args.pdf.endswith(".pdf"):
        print("‚ùå –£–∫–∞–∂–∏—Ç–µ –ø—Ä–∞–≤–∏–ª—å–Ω—ã–π –ø—É—Ç—å –∫ PDF —Ñ–∞–π–ª—É.")
    else:
        predict_from_pdf(args.pdf)