# import os
# from datasets import load_dataset
# from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments
# from transformers import Trainer
# from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training
# import torch
# from transformers.integrations import TensorBoardCallback

# MODEL_NAME = "meta-llama/Meta-Llama-3-8B-Instruct"  # –∏–ª–∏ micro –≤–µ—Ä—Å–∏—é

# def main():
#     os.environ["WANDB_DISABLED"] = "true"  # –æ—Ç–∫–ª—é—á–∞–µ–º wandb
#     device = "cuda" if torch.cuda.is_available() else "cpu"

#     dataset = load_dataset("json", data_files="data/llama_train.json", split="train")

#     tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=False)
#     model = AutoModelForCausalLM.from_pretrained(
#         MODEL_NAME,
#         torch_dtype=torch.float16,
#         device_map="auto",
#     )

#     model = prepare_model_for_kbit_training(model)

#     peft_config = LoraConfig(
#         r=8,
#         lora_alpha=16,
#         target_modules=["q_proj", "k_proj", "v_proj", "o_proj"],
#         lora_dropout=0.1,
#         bias="none",
#         task_type="CAUSAL_LM"
#     )

#     model = get_peft_model(model, peft_config)

#     def tokenize(example):
#         return tokenizer.apply_chat_template(example["messages"], return_tensors="pt", truncation=True, max_length=1024)

#     dataset = dataset.map(tokenize)

#     training_args = TrainingArguments(
#         output_dir="models/llama3-finetuned",
#         num_train_epochs=2,
#         per_device_train_batch_size=1,
#         gradient_accumulation_steps=4,
#         learning_rate=2e-4,
#         logging_dir="outputs/logs",
#         logging_steps=10,
#         save_strategy="epoch",
#         fp16=True,
#         report_to=[],
#     )

#     trainer = Trainer(
#         model=model,
#         args=training_args,
#         train_dataset=dataset,
#         tokenizer=tokenizer,
#         callbacks=[TensorBoardCallback()]
#     )

#     print("üîß –ù–∞—á–∏–Ω–∞–µ–º –æ–±—É—á–µ–Ω–∏–µ...")
#     trainer.train()

#     print("‚úÖ –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–æ–¥–µ–ª—å...")
#     trainer.save_model("models/llama3-finetuned")

# if __name__ == "__main__":
#     main()

# from datasets import load_dataset
# from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments, BitsAndBytesConfig
# from peft import LoraConfig
# from trl import SFTTrainer
# import torch

# # === –ú–æ–¥–µ–ª—å LLaMA-3 ===
# MODEL_NAME = "meta-llama/Meta-Llama-3-8B-Instruct"  # –æ—Ç–∫—Ä—ã—Ç—ã–µ –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤—ã: TinyLlama, OpenChat

# # === –î–∞—Ç–∞—Å–µ—Ç ===
# DATA_PATH = "data/llama_train.json"

# # === –ó–∞–≥—Ä—É–∂–∞–µ–º jsonl
# dataset = load_dataset("json", data_files=DATA_PATH, split="train")

# # === –°–∂–∏–º–∞–µ–º –≤–µ—Å –º–æ–¥–µ–ª–∏ –¥–ª—è —ç–∫–æ–Ω–æ–º–∏–∏ (LoRA + 4bit)
# bnb_config = BitsAndBytesConfig(
#     load_in_4bit=True,
#     bnb_4bit_quant_type="nf4",
#     bnb_4bit_use_double_quant=True,
#     bnb_4bit_compute_dtype=torch.bfloat16
# )

# # === –ó–∞–≥—Ä—É–∂–∞–µ–º —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä + –º–æ–¥–µ–ª—å
# tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True, padding_side="right")
# tokenizer.pad_token = tokenizer.eos_token

# # === –ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ–º –ª–æ—Ä—É
# lora_config = LoraConfig(
#     r=16,
#     lora_alpha=16,
#     target_modules=["q_proj", "k_proj", "v_proj", "o_proj"],
#     lora_dropout=0.1,
#     bias="none",
#     task_type="CAUSAL_LM"
# )

# # === –ú–æ–¥–µ–ª—å
# model = AutoModelForCausalLM.from_pretrained(
#     MODEL_NAME,
#     quantization_config=bnb_config,
#     device_map="auto"
# )

# # === –ê—Ä–≥—É–º–µ–Ω—Ç—ã –æ–±—É—á–µ–Ω–∏—è
# training_args = TrainingArguments(
#     output_dir="./models/llama3-finetuned",
#     num_train_epochs=2,
#     per_device_train_batch_size=1,
#     gradient_accumulation_steps=4,
#     logging_steps=10,
#     save_strategy="epoch",
#     learning_rate=2e-4,
#     fp16=True,
#     report_to=[],
# )


# # === –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è TRL-—Ç—Ä–µ–Ω–µ—Ä–∞
# trainer = SFTTrainer(
#     model=model,
#     args=training_args,
#     train_dataset=dataset,
#     packing=False,
#     tokenizer=tokenizer,
#     peft_config=lora_config,
#     dataset_text_field="messages",  # TRL-SFT —Å–∞–º –≤—ã–∑–æ–≤–µ—Ç tokenizer.apply_chat_template
# )

# trainer.train()
# trainer.save_model("models/llama3-finetuned")

# from datasets import load_dataset
# from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments, BitsAndBytesConfig
# from peft import LoraConfig
# from trl import SFTTrainer
# import torch

# # === –ú–æ–¥–µ–ª—å LLaMA-3 ===
# MODEL_NAME = "meta-llama/Meta-Llama-3-8B-Instruct"  # –∏–ª–∏ OpenChat

# # === –î–∞—Ç–∞—Å–µ—Ç ===
# DATA_PATH = "data/llama_train.json"

# # === –ó–∞–≥—Ä—É–∂–∞–µ–º jsonl
# dataset = load_dataset("json", data_files=DATA_PATH, split="train")

# # === –°–∂–∏–º–∞–µ–º –º–æ–¥–µ–ª—å LoRA + 4bit
# bnb_config = BitsAndBytesConfig(
#     load_in_4bit=True,
#     bnb_4bit_quant_type="nf4",
#     bnb_4bit_use_double_quant=True,
#     bnb_4bit_compute_dtype=torch.bfloat16
# )

# tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True, padding_side="right")
# tokenizer.pad_token = tokenizer.eos_token

# lora_config = LoraConfig(
#     r=16,
#     lora_alpha=16,
#     target_modules=["q_proj", "k_proj", "v_proj", "o_proj"],
#     lora_dropout=0.1,
#     bias="none",
#     task_type="CAUSAL_LM"
# )

# model = AutoModelForCausalLM.from_pretrained(
#     MODEL_NAME,
#     quantization_config=bnb_config,
#     device_map="auto"
# )

# training_args = TrainingArguments(
#     output_dir="./models/llama3-finetuned",
#     num_train_epochs=2,
#     per_device_train_batch_size=1,
#     gradient_accumulation_steps=4,
#     logging_steps=10,
#     save_strategy="epoch",
#     learning_rate=2e-4,
#     fp16=True,
#     report_to=[],
# )

# trainer = SFTTrainer(
#     model=model,
#     args=training_args,
#     train_dataset=dataset,
#     packing=False,
#     tokenizer=tokenizer,
#     peft_config=lora_config,
#     dataset_text_field="messages",  # TRL —Å–∞–º –≤—ã–∑–æ–≤–µ—Ç tokenizer.apply_chat_template
# )

# trainer.train()
# trainer.save_model("models/llama3-finetuned")

# print("‚úÖ –û–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ. –ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤ models/llama3-finetuned")

# from datasets import load_dataset
# from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments, BitsAndBytesConfig
# from peft import LoraConfig
# from trl import SFTTrainer
# import torch

# # ‚úÖ –ò—Å–ø–æ–ª—å–∑—É–µ–º –æ—Ç–∫—Ä—ã—Ç—É—é –º–æ–¥–µ–ª—å
# MODEL_NAME = "openchat/openchat-3.5-0106"
# DATA_PATH = "data/llama_train.json"

# dataset = load_dataset("json", data_files=DATA_PATH, split="train")

# bnb_config = BitsAndBytesConfig(
#     load_in_4bit=True,
#     bnb_4bit_quant_type="nf4",
#     bnb_4bit_use_double_quant=True,
#     bnb_4bit_compute_dtype=torch.bfloat16
# )

# tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)
# tokenizer.pad_token = tokenizer.eos_token

# lora_config = LoraConfig(
#     r=16,
#     lora_alpha=16,
#     target_modules=["q_proj", "k_proj", "v_proj", "o_proj"],
#     lora_dropout=0.1,
#     bias="none",
#     task_type="CAUSAL_LM"
# )

# model = AutoModelForCausalLM.from_pretrained(
#     MODEL_NAME,
#     device_map="auto",
#     torch_dtype=torch.float32
# )

# training_args = TrainingArguments(
#     output_dir="./models/openchat-finetuned",
#     num_train_epochs=2,
#     per_device_train_batch_size=1,
#     gradient_accumulation_steps=4,
#     logging_steps=10,
#     save_strategy="epoch",
#     learning_rate=2e-4,
#     fp16=True,
#     report_to=[],
# )

# trainer = SFTTrainer(
#     model=model,
#     args=training_args,
#     train_dataset=dataset,
#     dataset_text_field="messages",
#     peft_config=lora_config
# )

# trainer.train()
# trainer.save_model("models/openchat-finetuned")

# print("‚úÖ –ú–æ–¥–µ–ª—å —É—Å–ø–µ—à–Ω–æ –¥–æ–æ–±—É—á–µ–Ω–∞ –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞!")

# from trl import SFTTrainer
# from datasets import load_dataset
# from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments
# from peft import LoraConfig
# import torch

# MODEL_NAME = "openchat/openchat-3.5-0106"
# DATA_PATH = "data/llama_train.json"

# # –ó–∞–≥—Ä—É–∑–∫–∞ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞ –∏ –º–æ–¥–µ–ª–∏
# tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)
# tokenizer.pad_token = tokenizer.eos_token

# model = AutoModelForCausalLM.from_pretrained(
#     MODEL_NAME,
#     device_map="auto",
#     torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32
# )

# # –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞
# dataset = load_dataset("json", data_files=DATA_PATH, split="train")

# # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ PEFT
# lora_config = LoraConfig(
#     r=16,
#     lora_alpha=16,
#     target_modules=["q_proj", "k_proj", "v_proj", "o_proj"],
#     lora_dropout=0.1,
#     bias="none",
#     task_type="CAUSAL_LM"
# )

# # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –æ–±—É—á–µ–Ω–∏—è
# training_args = TrainingArguments(
#     output_dir="./models/llama3-finetuned",
#     num_train_epochs=2,
#     per_device_train_batch_size=1,
#     gradient_accumulation_steps=4,
#     logging_steps=10,
#     save_strategy="epoch",
#     learning_rate=2e-4,
#     fp16=True,
#     report_to=[],
# )

# # üëá –í–ê–ñ–ù–û: —Ç–≤–æ—è —Ñ—É–Ω–∫—Ü–∏—è, –ø—Ä–µ–æ–±—Ä–∞–∑—É—é—â–∞—è messages[] –≤ —Å—Ç—Ä–æ–∫—É
# def formatting_func(example):
#     return tokenizer.apply_chat_template(example["messages"], tokenize=False)

# # ‚úÖ –ù–æ–≤—ã–π –≤—ã–∑–æ–≤ SFTTrainer
# trainer = SFTTrainer(
#     model=model,
#     args=training_args,
#     train_dataset=dataset,
#     peft_config=lora_config,
#     formatting_func=formatting_func,
#     tokenizer=tokenizer,
# )

# # –û–±—É—á–µ–Ω–∏–µ
# trainer.train()
# trainer.save_model("models/llama3-finetuned")

# print("‚úÖ –û–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ.")

# from datasets import load_dataset
# from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments
# from peft import LoraConfig
# from trl import SFTTrainer
# import torch

# MODEL_NAME = "openchat/openchat-3.5-0106"
# DATA_PATH = "data/llama_train.json"

# # –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å –∏ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä
# tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)
# tokenizer.pad_token = tokenizer.eos_token

# model = AutoModelForCausalLM.from_pretrained(
#     MODEL_NAME,
#     device_map="auto",
#     torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32
# )

# # –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞—Ç–∞—Å–µ—Ç
# dataset = load_dataset("json", data_files=DATA_PATH, split="train")

# # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ LoRA
# lora_config = LoraConfig(
#     r=16,
#     lora_alpha=16,
#     target_modules=["q_proj", "k_proj", "v_proj", "o_proj"],
#     lora_dropout=0.1,
#     bias="none",
#     task_type="CAUSAL_LM"
# )

# # –ê—Ä–≥—É–º–µ–Ω—Ç—ã –æ–±—É—á–µ–Ω–∏—è
# training_args = TrainingArguments(
#     output_dir="./models/llama3-finetuned",
#     per_device_train_batch_size=1,
#     num_train_epochs=2,
#     gradient_accumulation_steps=4,
#     learning_rate=2e-4,
#     logging_steps=10,
#     save_strategy="epoch",
#     fp16=True,
#     report_to=[]
# )

# # üîß formatting_func –¥–ª—è apply_chat_template (–∑–∞–º–µ–Ω–∞ dataset_text_field)
# def format_func(example):
#     return tokenizer.apply_chat_template(example["messages"], tokenize=False)

# # ‚úÖ –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è SFTTrainer
# trainer = SFTTrainer(
#     model=model,
#     args=training_args,
#     train_dataset=dataset,
#     formatting_func=format_func,
#     peft_config=lora_config
# )

# # ==== –û–±—É—á–∞–µ–º –º–æ–¥–µ–ª—å ====
# trainer.train()

# # ==== –°–æ—Ö—Ä–∞–Ω—è–µ–º ====
# trainer.save_model("models/llama3-finetuned")

# print("‚úÖ –û–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ –∏ –º–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞.")

from datasets import load_dataset
from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments
from peft import LoraConfig, get_peft_model
from trl import SFTTrainer
import torch

MODEL_NAME = "openchat/openchat-3.5-0106"
DATA_PATH = "../data/llama_train.json"

# –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å –∏ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä
tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)
tokenizer.pad_token = tokenizer.eos_token

# –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å –Ω–∞ CPU/GPU, –±–µ–∑ –∫–≤–∞–Ω—Ç–æ–≤–∞–Ω–∏—è
model = AutoModelForCausalLM.from_pretrained(
    MODEL_NAME,
    torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32
)

# üîß –î–æ–±–∞–≤–ª—è–µ–º LoRA-–ø–æ–¥–¥–µ—Ä–∂–∫—É
lora_config = LoraConfig(
    r=16,
    lora_alpha=16,
    target_modules=["q_proj", "k_proj", "v_proj", "o_proj"],
    lora_dropout=0.1,
    bias="none",
    task_type="CAUSAL_LM"
)

model = get_peft_model(model, lora_config)

# –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞—Ç–∞—Å–µ—Ç
dataset = load_dataset("json", data_files=DATA_PATH, split="train")

# –ê—Ä–≥—É–º–µ–Ω—Ç—ã –æ–±—É—á–µ–Ω–∏—è
training_args = TrainingArguments(
    output_dir="../models/llama3-finetuned",
    per_device_train_batch_size=1,
    num_train_epochs=2,
    gradient_accumulation_steps=4,
    learning_rate=2e-4,
    logging_steps=10,
    save_strategy="epoch",
    fp16=torch.cuda.is_available(),
    report_to=[]
)

# –§—É–Ω–∫—Ü–∏—è –¥–ª—è —à–∞–±–ª–æ–Ω–∏–∑–∞—Ü–∏–∏
def format_func(example):
    return tokenizer.apply_chat_template(example["messages"], tokenize=False)

# –°–æ–∑–¥–∞—ë–º —Ç—Ä–µ–Ω–µ—Ä
trainer = SFTTrainer(
    model=model,
    args=training_args,
    train_dataset=dataset,
    formatting_func=format_func
)

trainer.train()
trainer.save_model("../models/llama3-finetuned")
tokenizer.save_pretrained("../models/llama3-complete")