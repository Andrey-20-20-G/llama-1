import torch
import torch.nn as nn
import joblib
import fitz
import re
import numpy as np
import os

DEVICE = "cuda" if torch.cuda.is_available() else "cpu"

# === –ü—É—Ç–∏ ===
MODEL_PATH = os.path.join("models", "cnn_model.pt")
VOCAB_PATH = os.path.join("models", "vocab.joblib")
MAX_SEQ_LEN = 300


# === –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ CNN (—Ç–∞–∫–∞—è –∂–µ –∫–∞–∫ –ø—Ä–∏ –æ–±—É—á–µ–Ω–∏–∏) ===
class CNNClassifier(nn.Module):
    def __init__(self, vocab_size, embed_dim=100, num_filters=128, kernel_size=5):
        super().__init__()
        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=0)
        self.conv = nn.Conv1d(embed_dim, num_filters, kernel_size)
        self.pool = nn.AdaptiveMaxPool1d(1)
        self.fc = nn.Linear(num_filters, 1)

    def forward(self, x):
        x = self.embedding(x).permute(0, 2, 1)  # (batch, embed_dim, seq_len)
        x = torch.relu(self.conv(x))
        x = self.pool(x).squeeze(-1)
        out = self.fc(x)
        return torch.sigmoid(out).squeeze()


# === –í—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏ ===

def extract_text_from_pdf(pdf_path):
    doc = fitz.open(pdf_path)
    text = " ".join([page.get_text() for page in doc])
    doc.close()
    return text.strip()


def clean_and_tokenize(text):
    text = re.sub(r"[^–∞-—è–ê-–Øa-zA-Z0-9—ë–Å\s]", "", text.lower())
    return text.split()


def text_to_tensor(text, vocab, max_len=MAX_SEQ_LEN):
    tokens = clean_and_tokenize(text)[:max_len]
    ids = [vocab.get(token, vocab.get("<UNK>", 1)) for token in tokens]
    padded = ids + [0] * (max_len - len(ids))
    return torch.tensor([padded], dtype=torch.long).to(DEVICE)


def interpret_probability(prob):
    if prob >= 0.9:
        return "üü• –ù–∞–ø–∏—Å–∞–Ω–æ –ò–ò"
    elif prob >= 0.65:
        return "üüß –ü—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–µ–Ω–Ω–æ –ò–ò"
    elif prob >= 0.35:
        return "üü® –ü—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–µ–Ω–Ω–æ —á–µ–ª–æ–≤–µ–∫"
    else:
        return "üü© –ù–∞–ø–∏—Å–∞–Ω–æ —á–µ–ª–æ–≤–µ–∫–æ–º"


# === –û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è ===

def run_inference(pdf_path):
    print("[INFO] –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ CNN –∏ —Å–ª–æ–≤–∞—Ä—è...")
    vocab = joblib.load(VOCAB_PATH)

    model = CNNClassifier(vocab_size=len(vocab)).to(DEVICE)
    model.load_state_dict(torch.load(MODEL_PATH, map_location=DEVICE))
    model.eval()

    print(f"[INFO] –ß—Ç–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –∏–∑ —Ñ–∞–π–ª–∞: {pdf_path}")
    text = extract_text_from_pdf(pdf_path)
    if not text.strip():
        print("[ERROR] PDF-—Ñ–∞–π–ª –ø—É—Å—Ç–æ–π –∏–ª–∏ —Ç–µ–∫—Å—Ç –Ω–µ –Ω–∞–π–¥–µ–Ω.")
        return

    input_tensor = text_to_tensor(text, vocab)

    with torch.no_grad():
        proba = model(input_tensor).item()

    label = interpret_probability(proba)

    print(f"\n‚úÖ –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è: {label}")
    print(f"üî¢ –í–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å, —á—Ç–æ –ò–ò: {proba:.4f}")


# === –ó–∞–ø—É—Å–∫ —Å–∫—Ä–∏–ø—Ç–∞ –∏–∑ –∫–æ–º–∞–Ω–¥–Ω–æ–π —Å—Ç—Ä–æ–∫–∏ ===

if __name__ == "__main__":
    import argparse

    parser = argparse.ArgumentParser(description="–ò–Ω—Ñ–µ—Ä–µ–Ω—Å CNN –º–æ–¥–µ–ª–∏ (PyTorch)")
    parser.add_argument("--pdf", required=True, help="–ü—É—Ç—å –∫ PDF —Ñ–∞–π–ª—É")
    args = parser.parse_args()

    run_inference(args.pdf)