import fitz  # pymupdf
import joblib
import os
import re

import numpy as np
import pandas as pd

from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize, sent_tokenize
import nltk
nltk.download('punkt')
nltk.download('stopwords')

from sklearn.feature_extraction.text import TfidfVectorizer
from scipy.sparse import hstack

from text_features import lexical_features  # Ğ¸Ğ· Ğ¿Ñ€ĞµĞ´Ñ‹Ğ´ÑƒÑ‰ĞµĞ³Ğ¾ Ğ¼Ğ¾Ğ´ÑƒĞ»Ñ


MODEL_PATH = "../models/rf_classifier.joblib"
SCALER_PATH = "../models/scaler.joblib"


def extract_text_from_pdf(pdf_path):
    doc = fitz.open(pdf_path)
    full_text = " ".join([page.get_text() for page in doc])
    doc.close()
    return full_text.strip()


def predict_class(text: str, tfidf_vocab: list, model, scaler):
    """
    Ğ’Ğ¾Ğ·Ğ²Ñ€Ğ°Ñ‰Ğ°ĞµÑ‚ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ğµ ĞºĞ»Ğ°ÑÑĞ° Ğ¸ Ğ²ĞµÑ€Ğ¾ÑÑ‚Ğ½Ğ¾ÑÑ‚ÑŒ
    """

    # 1. TF-IDF
    tfidf_vec = TfidfVectorizer(vocabulary=tfidf_vocab, stop_words='english')
    X_tfidf = tfidf_vec.fit_transform([text])

    # 2. Hand-crafted features
    feats = lexical_features(text)
    feat_array = scaler.transform([[
        feats['n_sentences'],
        feats['n_words'],
        feats['avg_word_length'],
        feats['avg_sent_length'],
        feats['unique_tokens'],
        feats['stopword_ratio']
    ]])

    # 3. Combine features
    x_combined = hstack([X_tfidf, feat_array])

    # 4. Prediction
    proba = model.predict_proba(x_combined)[0][1]  # Ğ²ĞµÑ€Ğ¾ÑÑ‚Ğ½Ğ¾ÑÑ‚ÑŒ "Ğ˜Ğ˜"
    pred_label = convert_proba_to_label(proba)

    return pred_label, proba


def convert_proba_to_label(prob: float) -> str:
    """
    Ğ˜Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ¸Ñ€ÑƒĞµÑ‚ Ğ²ĞµÑ€Ğ¾ÑÑ‚Ğ½Ğ¾ÑÑ‚ÑŒ Ğ² ĞºĞ°Ñ‚ĞµĞ³Ğ¾Ñ€Ğ¸Ñ.
    """
    if prob >= 0.9:
        return "ğŸŸ¥ ĞĞ°Ğ¿Ğ¸ÑĞ°Ğ½Ğ¾ Ğ˜Ğ˜"
    elif 0.65 <= prob < 0.9:
        return "ğŸŸ§ ĞŸÑ€ĞµĞ¸Ğ¼ÑƒÑ‰ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾ Ğ˜Ğ˜"
    elif 0.35 <= prob < 0.65:
        return "ğŸŸ¨ ĞŸÑ€ĞµĞ¸Ğ¼ÑƒÑ‰ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞº"
    else:
        return "ğŸŸ© ĞĞ°Ğ¿Ğ¸ÑĞ°Ğ½Ğ¾ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ¾Ğ¼"


def run_inference(pdf_path, tfidf_vocab_path="../models/tfidf_vocab.joblib"):
    print("[INFO] Ğ—Ğ°Ğ³Ñ€ÑƒĞ¶Ğ°ĞµĞ¼ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸...")
    model = joblib.load(MODEL_PATH)
    scaler = joblib.load(SCALER_PATH)
    tfidf_vocab = joblib.load(tfidf_vocab_path)

    print(f"[INFO] Ğ—Ğ°Ğ³Ñ€ÑƒĞ¶Ğ°ĞµĞ¼ Ñ„Ğ°Ğ¹Ğ» {pdf_path}...")
    text = extract_text_from_pdf(pdf_path)

    result, proba = predict_class(text, tfidf_vocab, model, scaler)

    print(f"\nâœ… ĞšĞ»Ğ°ÑÑĞ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ñ: {result}")
    print(f"ğŸ”¢ Ğ’ĞµÑ€Ğ¾ÑÑ‚Ğ½Ğ¾ÑÑ‚ÑŒ, Ñ‡Ñ‚Ğ¾ Ğ˜Ğ˜: {proba:.4f}")


if __name__ == "__main__":
    import argparse

    parser = argparse.ArgumentParser(description="ĞĞ½Ğ°Ğ»Ğ¸Ğ· PDF Ğ½Ğ° Ğ˜Ğ˜-Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¸")
    parser.add_argument("--pdf", type=str, required=True, help="ĞŸÑƒÑ‚ÑŒ Ğº PDF Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ñƒ")

    args = parser.parse_args()
    run_inference(args.pdf)